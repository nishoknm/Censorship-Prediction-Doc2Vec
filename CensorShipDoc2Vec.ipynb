{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Censorship Analysis using Doc2Vec\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Modules\n",
    "\n",
    "Used `gensim`, since `gensim` has a much more readable implementation of Word2Vec (and Doc2Vec). Also used `numpy` for general array manipulation, `stopwords` for removing insignificant words from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# random\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "# stop words from nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Format\n",
    "\n",
    "TODO : clean them up by converting everything to lower case and removing punctuation\n",
    "\n",
    "The result is to have two documents:\n",
    "\n",
    "- `train-blocked.txt`\n",
    "- `train-nonblocked.txt`\n",
    "\n",
    "Each of the sentence should be formatted as such:\n",
    "\n",
    "```\n",
    "Fang Lizhi was born on 12 February 1936 in Peking\n",
    "In 1948, one year before the PLA took over the city, as a student of the Beijing No.4 High School, Fang Lizhi joined an underground youth organization that was associated to CCP\n",
    "One of Fang Lizhi's extracurricular activities was assembling radio receivers from used parts\n",
    "\n",
    "```\n",
    "\n",
    "The sample up there contains three information sentences, each one taking up one entire line. Yes, **each document should be on one line, separated by new lines**. This is extremely important, because our parser depends on this to identify sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding Data to Doc2Vec\n",
    "\n",
    "Doc2Vec (the portion of `gensim` that implements the Doc2Vec algorithm) does a great job at word embedding. It only takes in `LabeledIndividualSentence` classes which basically yields `LabeledSentence`, a class from `gensim.models.doc2vec` representing a single sentence. Why the \"Labeled\" word? Well, here's how Doc2Vec differs from Word2Vec.\n",
    "\n",
    "Word2Vec simply converts a word into a vector.\n",
    "\n",
    "Doc2Vec not only does that, but also aggregates all the words in a sentence into a vector. To do that, it simply treats a sentence label as a special word, and does create a vector for that special word. Hence, that special word is a label for a sentence. \n",
    "\n",
    "So we have to format sentences into\n",
    "\n",
    "```python\n",
    "[['word1', 'word2', 'word3', 'lastword'], ['label1']]\n",
    "```\n",
    "\n",
    "`LabeledSentence` is simply a tidier way to do that. It contains a list of words, and a label for the sentence. We don't really need to care about how `LabeledSentence` works exactly, we just have to know that it stores those two things -- a list of words and a label.\n",
    "\n",
    "However, we need a way to convert our new line separated corpus into a collection of `LabeledSentence`s. The default constructor for the default `LabeledIndividualSentence` class in Doc2Vec can do that for a single text file, but can't do that for multiple files.\n",
    "\n",
    "So we write our own `LabeledIndividualSentence` class. The constructor takes in a dictionary that defines the files to read and the label prefixes sentences from that document should take on. Then, Doc2Vec can either read the collection directly via the iterator, or we can access the array directly. We also need a function to return a permutated version of the array of `LabeledSentence`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class LabeledIndividualSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    line = line.lower()\n",
    "                    tokens = utils.to_unicode(line).split()\n",
    "                    tokens = [w for w in tokens if not w in stopset]\n",
    "                    yield LabeledSentence(tokens, [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    line = line.lower() # change case for better results\n",
    "                    tokens = utils.to_unicode(line).split() # LabeledSentence accepts only unicode tokens\n",
    "                    tokens = [w for w in tokens if not w in stopset] # remove stopwords\n",
    "                    self.sentences.append(LabeledSentence(tokens, [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def getTag_words(self,words):\n",
    "        return [s for s in self.to_array() if ' '.join(s.words)==words][0].tags[0]\n",
    "    \n",
    "    def getWords_tag(self,tag):\n",
    "        return ' '.join([s for s in self.to_array() if s.tags[0]==tag][0].words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can feed the data files to `LabeledIndividualSentence`. As we mentioned earlier, `LabeledIndividualSentence` simply takes a dictionary with keys as the file names and values the special prefixes for sentences from that document. The prefixes need to be unique, so that there is no ambiguitiy for sentences from different documents.\n",
    "\n",
    "The prefixes will have a line number appended to them to label individual sentences in the documetns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'train-blocked.txt':'TRAIN_BL', 'train-nonblocked.txt':'TRAIN_NBL'}\n",
    "sentences = LabeledIndividualSentence(sources)\n",
    "alldocs = sentences.to_array()\n",
    "doc_list = alldocs[:]  # for reshuffling per pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Building the Vocabulary Table\n",
    "\n",
    "Doc2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them). So we feed it the array of sentences. `model.build_vocab` takes an array of `LabeledIndividualSentence`, hence our `to_array` function in the `LabeledIndividualSentence` class. \n",
    "\n",
    "More on Word2Vec documentation. Otherwise, here's a quick rundown:\n",
    "\n",
    "- `min_count`: ignore all words with total frequency lower than this. You have to set this to 1, since the sentence labels only appear once. Setting it any higher than 1 will miss out on the sentences.\n",
    "- `window`: the maximum distance between the current and predicted word within a sentence. Word2Vec uses a skip-gram model, and this is simply the window size of the skip-gram model.\n",
    "- `size`: dimensionality of the feature vectors in output. 100 is a good number. If you're extreme, you can go up to around 400.\n",
    "- `sample`: threshold for configuring which higher-frequency words are randomly downsampled\n",
    "- `workers`: use this many worker threads to train the model \n",
    "\n",
    "### Training Doc2Vec\n",
    "\n",
    "Now we train the model. The model is better trained if **in each training interval, the sequence of sentences fed to the model is randomized**.This is the reason for the `shuffle` method before each training.\n",
    "\n",
    "We train it for 20 interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "model.build_vocab(alldocs)\n",
    "for interval in range(20):\n",
    "    shuffle(doc_list) # reshuffling for better results\n",
    "    model.train(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Model\n",
    "\n",
    "Should produce sentence alternatives. One more interesting fact, blocked sentences move down the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word :  fang lizhi \n",
      "\n",
      "[('TRAIN_NBL_5', 0.027962500229477882), ('TRAIN_BL_1', -0.03432030230760574), ('TRAIN_NBL_7', -0.1078491061925888), ('TRAIN_NBL_3', -0.13212865591049194)]\n",
      "fang lizhi born 12 february 1936 peking \n",
      "\n",
      "tiananmen square massacre \n",
      "\n",
      "one fang lizhi's extracurricular activities assembling radio receivers used parts \n",
      "\n",
      "chinese government condemned tiananmen square protests counter-revolutionary riot, largely prohibited discussion remembrance events \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrd = \"fang lizhi\"\n",
    "print(\"Query word : \",wrd, \"\\n\")\n",
    "sims = model.docvecs.most_similar(sentences.getTag_words(wrd), topn=4)\n",
    "#print(sims)\n",
    "tags_sims = [s[0] for s in sims]\n",
    "for t in tags_sims:\n",
    "    print(sentences.getWords_tag(t),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 0.31396618485450745),\n",
       " ('deposed', 0.27445733547210693),\n",
       " ('demonstrators', 0.27362653613090515),\n",
       " ('initiated', 0.25295835733413696),\n",
       " ('june', 0.2520868480205536),\n",
       " ('fang', 0.23459796607494354),\n",
       " ('protests', 0.2315969616174698),\n",
       " ('residents,', 0.22675630450248718),\n",
       " ('secretary', 0.2199401557445526),\n",
       " ('place', 0.21217812597751617)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('lizhi', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
